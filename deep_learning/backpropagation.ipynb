{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915cf76c",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88501390",
   "metadata": {},
   "source": [
    "(예제) XOR 문제 딥러닝 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15eb1368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. XOR 데이터\n",
    "X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float)\n",
    "Y = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19fc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 모델 정의\n",
    "torch.manual_seed(530)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 2),    # 은닉층 : 입력 2 -> 출력 2 랜덤 초기화\n",
    "    nn.ReLU(),          # Relu 활성화 함수\n",
    "    nn.Linear(2, 1),    # 출력층(마지막) : 입력 2 -> 출력 1 랜덤 초기화\n",
    "    nn.Sigmoid()        # Sigmoid 활성화 함수 (확률처럼)\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69612c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7088\n",
      "Epoch 1, Loss: 0.7058\n",
      "Epoch 10000, Loss: 0.0017\n",
      "Epoch 20000, Loss: 0.0008\n",
      "Epoch 30000, Loss: 0.0005\n",
      "Epoch 40000, Loss: 0.0004\n",
      "Epoch 50000, Loss: 0.0003\n",
      "Epoch 60000, Loss: 0.0002\n",
      "Epoch 70000, Loss: 0.0002\n",
      "Epoch 80000, Loss: 0.0002\n",
      "Epoch 90000, Loss: 0.0002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Hidden_W</th>\n",
       "      <th>Hidden_b</th>\n",
       "      <th>Output_W</th>\n",
       "      <th>Output_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[-0.399, -0.457], [0.606, 0.699]]</td>\n",
       "      <td>[0.655, -0.051]</td>\n",
       "      <td>[[-0.647, 0.661]]</td>\n",
       "      <td>[-0.176]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[-0.407, -0.465], [0.603, 0.696]]</td>\n",
       "      <td>[0.645, -0.046]</td>\n",
       "      <td>[[-0.647, 0.655]]</td>\n",
       "      <td>[-0.178]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>[[-2.779, -2.779], [2.779, 2.779]]</td>\n",
       "      <td>[2.779, -2.779]</td>\n",
       "      <td>[[-4.763, -4.764]]</td>\n",
       "      <td>[5.855]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20000</td>\n",
       "      <td>[[-2.942, -2.942], [2.941, 2.941]]</td>\n",
       "      <td>[2.942, -2.941]</td>\n",
       "      <td>[[-5.047, -5.048]]</td>\n",
       "      <td>[6.613]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30000</td>\n",
       "      <td>[[-3.03, -3.03], [3.03, 3.03]]</td>\n",
       "      <td>[3.03, -3.03]</td>\n",
       "      <td>[[-5.202, -5.203]]</td>\n",
       "      <td>[7.046]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40000</td>\n",
       "      <td>[[-3.091, -3.091], [3.09, 3.09]]</td>\n",
       "      <td>[3.091, -3.09]</td>\n",
       "      <td>[[-5.307, -5.308]]</td>\n",
       "      <td>[7.349]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000</td>\n",
       "      <td>[[-3.136, -3.136], [3.136, 3.136]]</td>\n",
       "      <td>[3.136, -3.136]</td>\n",
       "      <td>[[-5.387, -5.388]]</td>\n",
       "      <td>[7.583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60000</td>\n",
       "      <td>[[-3.173, -3.173], [3.172, 3.172]]</td>\n",
       "      <td>[3.173, -3.173]</td>\n",
       "      <td>[[-5.451, -5.452]]</td>\n",
       "      <td>[7.774]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70000</td>\n",
       "      <td>[[-3.204, -3.204], [3.203, 3.203]]</td>\n",
       "      <td>[3.203, -3.203]</td>\n",
       "      <td>[[-5.504, -5.505]]</td>\n",
       "      <td>[7.934]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>80000</td>\n",
       "      <td>[[-3.23, -3.23], [3.229, 3.229]]</td>\n",
       "      <td>[3.23, -3.229]</td>\n",
       "      <td>[[-5.55, -5.551]]</td>\n",
       "      <td>[8.073]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90000</td>\n",
       "      <td>[[-3.252, -3.252], [3.252, 3.252]]</td>\n",
       "      <td>[3.252, -3.252]</td>\n",
       "      <td>[[-5.59, -5.591]]</td>\n",
       "      <td>[8.194]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch                            Hidden_W         Hidden_b  \\\n",
       "0       0  [[-0.399, -0.457], [0.606, 0.699]]  [0.655, -0.051]   \n",
       "1       1  [[-0.407, -0.465], [0.603, 0.696]]  [0.645, -0.046]   \n",
       "2   10000  [[-2.779, -2.779], [2.779, 2.779]]  [2.779, -2.779]   \n",
       "3   20000  [[-2.942, -2.942], [2.941, 2.941]]  [2.942, -2.941]   \n",
       "4   30000      [[-3.03, -3.03], [3.03, 3.03]]    [3.03, -3.03]   \n",
       "5   40000    [[-3.091, -3.091], [3.09, 3.09]]   [3.091, -3.09]   \n",
       "6   50000  [[-3.136, -3.136], [3.136, 3.136]]  [3.136, -3.136]   \n",
       "7   60000  [[-3.173, -3.173], [3.172, 3.172]]  [3.173, -3.173]   \n",
       "8   70000  [[-3.204, -3.204], [3.203, 3.203]]  [3.203, -3.203]   \n",
       "9   80000    [[-3.23, -3.23], [3.229, 3.229]]   [3.23, -3.229]   \n",
       "10  90000  [[-3.252, -3.252], [3.252, 3.252]]  [3.252, -3.252]   \n",
       "\n",
       "              Output_W  Output_b  \n",
       "0    [[-0.647, 0.661]]  [-0.176]  \n",
       "1    [[-0.647, 0.655]]  [-0.178]  \n",
       "2   [[-4.763, -4.764]]   [5.855]  \n",
       "3   [[-5.047, -5.048]]   [6.613]  \n",
       "4   [[-5.202, -5.203]]   [7.046]  \n",
       "5   [[-5.307, -5.308]]   [7.349]  \n",
       "6   [[-5.387, -5.388]]   [7.583]  \n",
       "7   [[-5.451, -5.452]]   [7.774]  \n",
       "8   [[-5.504, -5.505]]   [7.934]  \n",
       "9    [[-5.55, -5.551]]   [8.073]  \n",
       "10   [[-5.59, -5.591]]   [8.194]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 학습\n",
    "records = []\n",
    "\n",
    "for epoch in range(100000):\n",
    "    output = model(X)           # 예측값 계산\n",
    "    loss = loss_fn(output, Y)   # 손실 계산\n",
    "\n",
    "    optimizer.zero_grad()       # 기울기 초기화\n",
    "    loss.backward()             # 역전파\n",
    "    optimizer.step()            # 파라미터 업데이트\n",
    "\n",
    "    if epoch % 10000 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "        records.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"hidden_weight\": model[0].weight.detach().numpy().copy(),\n",
    "            \"hidden_bias\": model[0].bias.detach().numpy().copy(),\n",
    "            \"output_weight\": model[2].weight.detach().numpy().copy(),\n",
    "            \"output_bias\": model[2].bias.detach().numpy().copy(),\n",
    "        })\n",
    "\n",
    "df_records = pd.DataFrame([\n",
    "    {\n",
    "        \"Epoch\": r[\"epoch\"],\n",
    "        \"Hidden_W\": r[\"hidden_weight\"].round(3),\n",
    "        \"Hidden_b\": r[\"hidden_bias\"].round(3),\n",
    "        \"Output_W\": r[\"output_weight\"].round(3),\n",
    "        \"Output_b\": r[\"output_bias\"].round(3),\n",
    "    }\n",
    "    for r in records\n",
    "])\n",
    "\n",
    "df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f839ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "예측 결과:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# 4. 결과 확인\n",
    "with torch.no_grad():\n",
    "    output = model(X)\n",
    "    predicted = torch.round(output)  # 0.5 기준 반올림 (이진 분류)\n",
    "    print(\"\\n예측 결과:\")\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9743c51",
   "metadata": {},
   "source": [
    "### XOR 모델 학습 과정 정리 : Epoch 0 → 1\n",
    "\n",
    "> ### 초기 파라미터 (Epoch 0)\n",
    "- layer1 : 은닉층\n",
    "- layer2 (마지막) : 출력층\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_1 = \\begin{bmatrix}\n",
    "-0.399 & -0.457 \\\\\n",
    "0.606 & 0.699\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b}_1 = \\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "-0.051\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_2 = \\begin{bmatrix}\n",
    "-0.647 & 0.661\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b}_2 = \\begin{bmatrix}\n",
    "-0.176\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "> ### 순전파 (Forward)\n",
    "\n",
    "**1. 입력 $x$ (4개 샘플, 4 by 2)**\n",
    "\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**2. 은닉층 선형합: $z_1 = x W_1^{T} + b_1$** <br>\n",
    "(참고, xW와 b가 차원이 안 맞지만 토치의 브로드캐스팅 통해 연산 가능)\n",
    "\n",
    "$$\n",
    "z_1 = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-0.399 & 0.606 \\\\\n",
    "-0.457 & 0.699\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.655 & -0.051\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "※ $b_1$는 shape이 $(2,)$이지만 **브로드캐스팅**에 의해 각 샘플마다 더해짐\n",
    "\n",
    "$$\n",
    "z_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 & -0.051 \\\\\n",
    "0.198 & 0.648 \\\\\n",
    "0.256 & 0.555 \\\\\n",
    "-0.201 & 1.254\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**3. ReLU 적용: $h = ReLU(z_1)$**\n",
    "\n",
    "$$\n",
    "h =\n",
    "\\begin{bmatrix}\n",
    "0.655 & 0.000 \\\\\n",
    "0.198 & 0.648 \\\\\n",
    "0.256 & 0.555 \\\\\n",
    "0.000 & 1.254\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**4. 출력층 선형합: $z_2 = h W_2^{T} + b_2$**\n",
    "\n",
    "$$\n",
    "z_2 = \n",
    "\\begin{bmatrix}\n",
    "0.655 & 0.000 \\\\\n",
    "0.198 & 0.648 \\\\\n",
    "0.256 & 0.555 \\\\\n",
    "0.000 & 1.254\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-0.647 \\\\\n",
    "0.661\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "-0.176\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.5998 \\\\\n",
    "0.1242 \\\\\n",
    "0.0252 \\\\\n",
    "0.6529\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**5. 출력값: $y_{pred} = sigmoid(z_2)$**\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{bmatrix}\n",
    "0.3544 \\\\\n",
    "0.5310 \\\\\n",
    "0.5063 \\\\\n",
    "0.6577\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**6. 정답 $y$**\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "> ### 손실 계산 (Loss)\n",
    "\n",
    "- 여기선 **Binary Cross Entropy Loss** 사용 (정답과 예측이 다르면 손실이 커짐을 정답이 1,0일 때 조건부 수식으로 설명)  \n",
    "      - 정답 y = 1일 떄, L = -\\log(\\hat{y}) : 정답이 1인데, 예측한 확률이 낮으면 손실이 크게 계산됨  \n",
    "      - 정답 y = 0일 때, L = -\\log(1 - \\hat{y}) : 정답이 0인데, 예측한 확률이 높으면 손실이 크게 계산됨\n",
    "\n",
    "\\begin{aligned}\n",
    "L &= -\\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\\\\n",
    "&= -\\frac{1}{4} \\left[\n",
    "(0)\\log(0.3544) + (1)\\log(1 - 0.3544) \\right. \\\\\n",
    "&\\quad + (1)\\log(0.5310) + (0)\\log(1 - 0.5310) \\\\\n",
    "&\\quad + (1)\\log(0.5063) + (0)\\log(1 - 0.5063) \\\\\n",
    "&\\quad + (0)\\log(0.6577) + (1)\\log(1 - 0.6577) \\left. \\right] \\\\\n",
    "&= -\\frac{1}{4} \\left[\n",
    "\\log(1 - 0.3544) + \\log(0.5310) + \\log(0.5063) + \\log(1 - 0.6577)\n",
    "\\right] \\\\\n",
    "&= -\\frac{1}{4} \\left[\n",
    "\\log(0.6456) + \\log(0.5310) + \\log(0.5063) + \\log(0.3423)\n",
    "\\right] \\\\\n",
    "&= 0.7058\n",
    "\\end{aligned}\n",
    "\n",
    "---\n",
    "\n",
    "> ### 역전파 (Backpropagation)\n",
    "- y와 y_hat이 같기를 궁극적으로 목표\n",
    "- gradient(미분, 기울기)를 활용하여 w와 b 업데이트  \n",
    "      - ∂L / ∂W : w를 살짝 늘렸을 때, Loss가 얼마나 늘어나는지  \n",
    "      - 기울기가 양수면, w를 줄여야 Loss도 감소 / 음수면, w를 늘려야 Loss가 감소 (-> 기울기*a 빼야함)\n",
    "- 여러 레이어가 있을 때 기울기를 구하는 방식은 **Chain Rule**  \n",
    "      - 출력 -> 입력 방향으로 미분을 계속 곱해나가며 역전파 수행  \n",
    "      - e.g. $\\frac{dL}{dx} = \\frac{dL}{dy} \\cdot \\frac{dy}{dz} \\cdot \\frac{dz}{dx}$\n",
    "\n",
    "\n",
    "**우리가 구해야 하는 파라미터는 w1, w2, b1, b2로, 그에 대한 gradient $\\frac{\\partial L}{\\partial W_1}, \\frac{\\partial L}{\\partial W_2}, \\frac{\\partial L}{\\partial b_2}, \\frac{\\partial L}{\\partial b_2}$를 구해야 함**\n",
    "\n",
    "**0. 구조 다시 보기**\n",
    "$$\n",
    "입력 x \\\\\n",
    "↓\\\\\n",
    "은닉층: z₁ = x @ W₁ + b₁\\\\\n",
    "↓ (ReLU)\\\\\n",
    "h = ReLU(z₁)\\\\\n",
    "↓\\\\\n",
    "출력층: z₂ = h @ W₂ + b₂\\\\\n",
    "↓ (Sigmoid)\\\\\n",
    "ŷ = sigmoid(z₂)\\\\\n",
    "↓\\\\\n",
    "Loss = BCE(ŷ, y)\n",
    "$$\n",
    "\n",
    "\n",
    "**1. GRADIENT 계산**\n",
    "\n",
    "### 출력층 오차 (BCE + Sigmoid)\n",
    "\n",
    "손실 함수 (Binary Cross Entropy):\n",
    "\n",
    "$$\n",
    "L = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "출력층에서의 예측값은 sigmoid 함수:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z_2) = \\frac{1}{1 + e^{-z_2}}\n",
    "$$\n",
    "\n",
    "Chain Rule 적용:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2}\n",
    "$$\n",
    "\n",
    "각 항 미분:\n",
    "\n",
    "1. BCE 미분:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}}\n",
    "$$\n",
    "\n",
    "2. Sigmoid 미분 (1+e^{-z_2}를 t로 치환 후 미분):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}}{\\partial z_2} = \\hat{y}(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "두 미분을 곱하면:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_2} =\n",
    "\\left( -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} \\right)\n",
    "\\cdot \\hat{y}(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "이걸 정리하면:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_2} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "출력층의 오차는 다음과 같이 계산할 수 있음 (BCE + Sigmoid 많이 나오는 조합)  \n",
    "($\\delta_2$ : (delta) 로컬 그레디언트로 $\\delta_n$ = $\\frac{\\partial L}{\\partial z_n})$\n",
    "\n",
    "$$\n",
    "\\delta_2 = \\frac{\\partial L}{\\partial z_2} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "\n",
    "### ✅ 가중치 기울기: $\\frac{\\partial L}{\\partial W_2}$\n",
    "\n",
    "체인 룰을 이용해:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2}\n",
    "= \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial W_2}\n",
    "= h^T \\cdot \\delta_2\n",
    "$$\n",
    "\n",
    "### ✅ 편향 기울기: $\\frac{\\partial L}{\\partial b_2}$\n",
    "\n",
    "편향은 각 샘플마다 동일하게 적용되므로, 오차를 단순히 합산:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_2} = \\sum_{i=1}^{N} \\delta_2^{(i)}\n",
    "$$\n",
    "\n",
    "### 은닉층 오차 (Relu)\n",
    "\n",
    "은닉층의 선형 계산:\n",
    "\n",
    "$$\n",
    "z_1 = x \\cdot W_1 + b_1\n",
    "$$\n",
    "\n",
    "활성화 함수로 ReLU 사용:\n",
    "\n",
    "$$\n",
    "h = \\text{ReLU}(z_1)\n",
    "$$\n",
    "\n",
    "\n",
    "출력층에서 계산한 오차 $\\delta_2$를 기반으로  \n",
    "은닉층으로 오차를 전파 (chain rule):\n",
    "\n",
    "$$\n",
    "\\delta_1 = \\left( \\delta_2 \\cdot W_2^T \\right) \\circ \\text{ReLU}'(z_1)\n",
    "$$\n",
    "\n",
    "- $\\text{ReLU}'(z_1)$: $z_1 > 0$일 때는 1, 아니면 0\n",
    "\n",
    "\n",
    "### ✅ 가중치 기울기: $\\frac{\\partial L}{\\partial W_1}$\n",
    "\n",
    "은닉층 가중치의 기울기는:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1}\n",
    "= \\frac{\\partial L}{\\partial z_2}\n",
    "\\cdot \\frac{\\partial z_2}{\\partial h}\n",
    "\\cdot \\frac{\\partial h}{\\partial z_1}\n",
    "\\cdot \\frac{\\partial z_1}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "정리해서,\n",
    "\n",
    "$$\n",
    "\\delta_1 = \\left( \\delta_2 \\cdot W_2^T \\right) \\circ \\text{ReLU}'(z_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = x^T \\cdot \\delta_1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### ✅ 편향 기울기: $\\frac{\\partial L}{\\partial b_1}$\n",
    "\n",
    "편향은 샘플별로 동일하게 적용되므로 오차를 합산:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_1} = \\sum_{i=1}^{N} \\delta_1^{(i)}\n",
    "$$\n",
    "\n",
    "**2. PARAMETER 업데이트**\n",
    "\n",
    "학습률 $\\eta$를 이용한 파라미터 업데이트:\n",
    "\n",
    "$$\n",
    "W_2 \\leftarrow W_2 - \\eta \\cdot \\frac{\\partial L}{\\partial W_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_2 \\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_1 \\leftarrow W_1 - \\eta \\cdot \\frac{\\partial L}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_1 \\leftarrow b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40cfd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889504af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0691\n",
      "Epoch 1, Loss: 0.5375\n",
      "Epoch 10000, Loss: 0.0003\n",
      "Epoch 20000, Loss: 0.0002\n",
      "Epoch 30000, Loss: 0.0001\n",
      "Epoch 40000, Loss: 0.0001\n",
      "Epoch 50000, Loss: 0.0001\n",
      "Epoch 60000, Loss: 0.0001\n",
      "Epoch 70000, Loss: 0.0000\n",
      "Epoch 80000, Loss: 0.0000\n",
      "Epoch 90000, Loss: 0.0000\n",
      "\n",
      "예측 결과:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 정의 (XOR 문제)\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])  # (4, 2)\n",
    "Y = np.array([[0],[1],[1],[0]])          # (4, 1)\n",
    "\n",
    "# 2. 하이퍼파라미터 및 초기화\n",
    "np.random.seed(530)\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# 가중치 초기화\n",
    "W1 = np.random.randn(input_size, hidden_size)  # 표준 정규분포 N(0,1) : 적당히 0 근처에서 출력 및 그레디언트 안정성 확보\n",
    "b1 = np.zeros((1, hidden_size))  # 바이어스는 0으로 해도 알아서 학습으로 조절됨\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# 활성화 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# 손실 함수 (Binary Cross Entropy)\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-7  # log(0) 방지\n",
    "    return -np.mean(y_true*np.log(y_pred + eps) + (1 - y_true)*np.log(1 - y_pred + eps))\n",
    "\n",
    "# 3. 학습\n",
    "for epoch in range(100000):\n",
    "    # 순전파\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = binary_cross_entropy(a2, Y)\n",
    "\n",
    "    # 역전파\n",
    "    d_loss = a2 - Y  # BCE + sigmoid 출력의 도함수\n",
    "    dW2 = np.dot(a1.T, d_loss)\n",
    "    db2 = np.sum(d_loss, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(d_loss, W2.T)\n",
    "    dz1 = da1 * relu_deriv(z1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "    # 출력\n",
    "    if epoch % 10000 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# 4. 결과 확인 (최종)\n",
    "a2 = sigmoid(np.dot(relu(np.dot(X, W1) + b1), W2) + b2)\n",
    "pred = np.round(a2)\n",
    "print(\"\\n예측 결과:\")\n",
    "print(pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
